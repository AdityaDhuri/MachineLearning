import pandas as pd
from dbfread import DBF
import pandas as pd
import numpy as np
import glob
from datetime import datetime
import boto3
import s3fs
import csv, time, logging
import warnings
import os
import psycopg2

warnings.filterwarnings("ignore")

date = datetime.today()
curr_year = date.strftime("%Y")
curr_month = date.strftime("%Y%m")
local_dir='./download/'

s3 = boto3.resource('s3',
                    aws_access_key_id='AKIA42PJXDPV6PR5L46P',
                    aws_secret_access_key='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')

connection = psycopg2.connect(user="sap_postgres_rds",
                              password="Sapient123",
                              host="mum-pgsql-rds-db-1.cfixywydcw2x.ap-south-1.rds.amazonaws.com",
                              port="5432",
                              database="mum_sap_pgsql_db")

table_extract_query = "SELECT s3bucket,s3folderpath,sourcesystemname,sourcefilename,file_id FROM control_master_file_tablelist where sourcesystemname IN ('KARVY','CAMS') and active_flag='Y';"

table_extract_query_df = pd.read_sql(table_extract_query, connection)
table_count = len(table_extract_query_df)

bucketname = "mum-dev-sap-on-premises-data-sftp"
my_bucket = s3.Bucket(bucketname)
source="inb_cams/transaction/dbf/"

objects = list(my_bucket.objects.filter(Prefix=source))
while True:

    for obj in objects:
        print(obj.key)

        class ec2_download_folder_dbf_to_csv_upload_s3(object):

            def __init__(self,logger=None, verbose=True):
                if not logger:
                    logger = logging.getLogger(__name__)
                    logger.setLevel(logging.DEBUG)
                    if verbose:
                        h = logging.StreamHandler()
                        h.setFormatter(logging.Formatter("%(levelname)s - %(message)s"))
                        logger.addHandler(h)
                    else:
                        logger.addHandler(logging.NullHandler())
                self._logger = logger

            def download_s3_folder(self,bucket_name, s3_folder, local_dir):

                    bucket = s3.Bucket(bucket_name)
                    for obj in bucket.objects.filter(Prefix=s3_folder):
                        src_obj=obj.key

                        dest_path = local_dir + src_obj
                        target = dest_path if local_dir is not None \
                            else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))
                        if not os.path.exists(os.path.dirname(target)):
                            os.makedirs(os.path.dirname(target))
                        if obj.key[-1] == '/':
                            continue
                        bucket.download_file(obj.key, target)
        upload_obj = ec2_download_folder_dbf_to_csv_upload_s3 ()  #class created

        for i in range(table_count):
            bucket_name = str(table_extract_query_df['s3bucket'].iloc[i]).strip()
            sourcesystem = str(table_extract_query_df['sourcesystemname'].iloc[i]).strip()
            s3folder = str(table_extract_query_df['s3folderpath'].iloc[i]).strip()
            file_id = str(table_extract_query_df['file_id'].iloc[i]).strip()
            file_name = str(table_extract_query_df['sourcefilename'].iloc[i]).strip()
            print(file_name)
            s3_resource = boto3.resource('s3', aws_access_key_id='AKIA42PJXDPV6PR5L46P',
                                         aws_secret_access_key='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')

            bucket = s3_resource.Bucket(bucket_name)

            count = bucket.objects.filter(Prefix=s3folder)

            print(len(list(count)))
            length = len(list(count))
            if length != 1:
                print("[Information] %s directory is downloading" % s3folder)
                upload_obj.download_s3_folder(bucket_name,s3folder,local_dir)


        date = datetime.today()
        datenow = date.strftime("%Y-%m-%d")
        curr_year = date.strftime("%Y")
        curr_month = date.strftime("%Y%m")
        curr_date = date.strftime("%Y/%m/%d")

        table_extract_query = "SELECT s3bucket,s3foldererror,s3folderpath,s3folderpathcsv,s3folderpatharchive,sourcesystemname,sourcefilename,destinationtablename,file_id, del_files FROM control_master_file_tablelist where sourcesystemname IN ('KARVY','CAMS') and active_flag='Y';"

        table_extract_query_df = pd.read_sql(table_extract_query, connection)
        print(table_extract_query_df)
        table_count = len(table_extract_query_df)
        print(table_count)

        class dbf_csv(object):

            def __init__(self,logger=None, verbose=True):
                if not logger:
                    logger = logging.getLogger(__name__)
                    logger.setLevel(logging.DEBUG)
                    if verbose:
                        h = logging.StreamHandler()
                        h.setFormatter(logging.Formatter("%(levelname)s - %(message)s"))
                        logger.addHandler(h)
                    else:
                        logger.addHandler(logging.NullHandler())

                self._logger = logger

            ################################### MOVING THE FILE WITH COMPARISON #######################
            def inbound_dbf_to_archive_move_file(self, bucketname,s3foldererror, s3folder, s3folderpatharchive,file_err):
                srcbucket = s3.Bucket(bucketname)

                for object in srcbucket.objects.filter(Prefix=s3folder):
                    source_filename = (object.key).split('/')[-1]
                    print("source_filename source_filename source_filename", source_filename)

                    if source_filename == file_err:
                        copy_source = {
                            'Bucket': srcbucket.name,
                            'Key': object.key
                        }
                        target_filename = "{}".format(s3foldererror + '/' + str(datenow) + '/' + source_filename)

                        s3.meta.client.copy(copy_source, bucketname, target_filename)
                        srcKey = object.key
                        if not srcKey.endswith('/'):
                            s3.Object(bucketname, srcKey).delete()

                    else:
                        copy_source = {
                            'Bucket': srcbucket.name,
                            'Key': object.key
                        }
                        target_filename = "{}".format(s3folderpatharchive + '/' + str(datenow) + '/' + source_filename)

                        s3.meta.client.copy(copy_source, bucketname, target_filename)
                        srcKey = object.key
                        if not srcKey.endswith('/'):
                            s3.Object(bucketname, srcKey).delete()

            def upload_dbf_csv(self,s3folder,s3folderpathcsv,file_name,sourcesystem,bucketname,file_id):
                cursor = connection.cursor()
                path2="C:/pycharm_sapient/code/download" +'/'+s3folder+"/*.dbf"

                print("path2",path2)
                files = glob.glob(path2)
                print(files)
                frame = []
                for file in files:
                    print(file)
                    file_err = file.split("\\")[-1]
                    print(file_err)
                    path_parts = file.split('/')
                    file_name = path_parts[-1]
                    file_name_f= file_name.split('.')[0]
                    file_name_fin = file_name_f.split('\\')[1]


                    print("file_id:", file_id)

                    print('test')
                    filesize_var = os.path.getsize(file)
                    print('@@@@@@@@@@ FILE SIZE @@@@@@@@@@@@@@',filesize_var)

                    try:
                        if filesize_var !=0:

                            dbf1 = DBF(file, encoding='ISO-8859-1')
                            print('test1')

                            frame.append(pd.DataFrame(iter(dbf1)))
                            df = pd.concat(frame)
                            df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                            df = df.replace('\n', ' ', regex=True).astype(str)
                            df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
                            df.replace(to_replace=[r"\\n|\\r", "\n|\r"], value=["", ""], regex=True, inplace=True)
                            count=len(df)
                            src_count = len(df)
                            print(count)
                            print(df.head())
                            fs = s3fs.S3FileSystem(key='AKIA42PJXDPV6PR5L46P',
                                               secret='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')

                            path = "s3://" + bucketname + "/" + s3folderpathcsv + '/' + file_name_fin + '.csv'
                            path1 = file_name_fin + '.csv'
                            print("file_name", file_name_fin)
                            print("folder path",s3folderpathcsv)

                            job_detail_insrt_qry = "INSERT INTO job_detail (sno, job_id, job_name, object_name, object_size, job_status, start_time, end_time, duration_secs, error_msg, src_cnt, target_cnt, sourcesystem, data_load_env, business_date, ins_cnt, upd_cnt, del_cnt, rej_cnt, file_id, destinationtablename) VALUES (DEFAULT,1,'dbf_file_to_csv file ', '" + path + "', 100, 'Running',CURRENT_TIMESTAMP, NULL, NULL, NULL,NULL, NULL,'" + sourcesystem + "', 'rawdata',current_date, NULL, NULL, NULL, NULL,'" + file_id +"' ,NULL)"

                            cursor.execute(job_detail_insrt_qry)

                            with fs.open(path, 'wb', encoding='utf-8') as f:
                                np.savetxt(f, df,  delimiter='*~|', header='*~|'.join(df.columns.values), fmt='%s', comments='', encoding="utf-8")

                                job_detail_updt_qry = "UPDATE job_detail SET job_status='Completed',end_time=current_timestamp, duration_secs =NULL, src_cnt =" + str(
                                    src_count) + " WHERE object_name='" + path + "' AND business_date = current_date"
                                job_detail_dur_qry = "UPDATE job_detail SET duration_secs=extract(epoch from (start_time::timestamp - end_time::timestamp)) WHERE object_name='" + path + "' AND business_date = current_date"

                                frame = []
                                os.remove(file)
                                cursor.execute(job_detail_updt_qry)
                                cursor.execute(job_detail_dur_qry)
                                connection.commit()

                                print("successful uploaded")
                        else :
                            self.inbound_dbf_to_archive_move_file(bucketname,s3foldererror, s3folder,s3folderpatharchive,file_err)

                    except  Exception as e:
                        print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')
                        print(file)
                        self.inbound_dbf_to_archive_move_file(bucketname,s3foldererror, s3folder,s3folderpatharchive, file_err)


        convert_csv=dbf_csv()
        for i in range(table_count):
            sourcesystem = str(table_extract_query_df['sourcesystemname'].iloc[i]).strip()

            bucketname = str(table_extract_query_df['s3bucket'].iloc[i]).strip()
            s3folder = str(table_extract_query_df['s3folderpath'].iloc[i]).strip()
            s3folderpathcsv = str(table_extract_query_df['s3folderpathcsv'].iloc[i]).strip()
            s3folderpatharchive = str(table_extract_query_df['s3folderpatharchive'].iloc[i]).strip()
            s3foldererror = str(table_extract_query_df['s3foldererror'].iloc[i]).strip()
            file_id = str(table_extract_query_df['file_id'].iloc[i]).strip()
            file_name = str(table_extract_query_df['sourcefilename'].iloc[i]).strip()

            convert_csv.upload_dbf_csv(s3folder,s3folderpathcsv,file_name,sourcesystem,bucketname,file_id)
            convert_csv.inbound_dbf_to_archive_move_file(bucketname,s3foldererror, s3folder,s3folderpatharchive,file_err='NULL')

        cursor = connection.cursor()
        dir = "download"
        location = "C:/pycharm_sapient/code"
        path_dir = os.path.join(location, dir)
        print("% s has been removed successfully" % dir)
    time.sleep(5)